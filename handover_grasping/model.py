# AUTOGENERATED! DO NOT EDIT! File to edit: 01_grasping_estimation.ipynb (unless otherwise specified).

__all__ = ['HANet', 'HANet_depth']

# Cell
import torch
import torch.nn as nn
import torchvision
import copy
import cv2
import sys
import numpy as np
from .utils import get_model

class HANet(nn.Module):
    class FCN_model(nn.Module):
        def __init__(self, n_classes=4):
            super(HANet.FCN_model, self).__init__()
            self.color_trunk = torchvision.models.resnet101(pretrained=True)
            del self.color_trunk.fc, self.color_trunk.avgpool, self.color_trunk.layer4
            self.depth_trunk = copy.deepcopy(self.color_trunk)
            self.conv1 = nn.Conv2d(2048, 512, 1)
            self.conv2 = nn.Conv2d(512, 128, 1)
            self.conv3 = nn.Conv2d(128, n_classes, 1)
        def forward(self, color, depth):
            # Color
            color_feat_1 = self.color_trunk.conv1(color) # 3 -> 64
            color_feat_1 = self.color_trunk.bn1(color_feat_1)
            color_feat_1 = self.color_trunk.relu(color_feat_1)
            color_feat_1 = self.color_trunk.maxpool(color_feat_1)
            color_feat_2 = self.color_trunk.layer1(color_feat_1) # 64 -> 256
            color_feat_3 = self.color_trunk.layer2(color_feat_2) # 256 -> 512
            color_feat_4 = self.color_trunk.layer3(color_feat_3) # 512 -> 1024
            # Depth
            depth_feat_1 = self.depth_trunk.conv1(depth) # 3 -> 64
            depth_feat_1 = self.depth_trunk.bn1(depth_feat_1)
            depth_feat_1 = self.depth_trunk.relu(depth_feat_1)
            depth_feat_1 = self.depth_trunk.maxpool(depth_feat_1)
            depth_feat_2 = self.depth_trunk.layer1(depth_feat_1) # 64 -> 256
            depth_feat_3 = self.depth_trunk.layer2(depth_feat_2) # 256 -> 512
            depth_feat_4 = self.depth_trunk.layer3(depth_feat_3) # 512 -> 1024
            # Concatenate
            feat = torch.cat([color_feat_4, depth_feat_4], dim=1) # 2048
            feat_1 = self.conv1(feat)
            feat_2 = self.conv2(feat_1)
            feat_3 = self.conv3(feat_2)
            return nn.Upsample(scale_factor=2, mode="bilinear")(feat_3)

    def __init__(self, pretrained=False, n_class=4):
        super(HANet, self).__init__()
        if pretrained == True:
            self.net = self.FCN_model(n_classes=4)
            model_path = get_model()
            self.net.load_state_dict(torch.load(model_path))
            print('Load pretrained complete')
        else:
            self.net = self.FCN_model(n_classes=n_class)

    def load(self, model_path):
        self.net.load_state_dict(torch.load(model_path))
        print('Load pretrained complete')

    def get_affordanceMap(self, input_color, input_depth, depth_origin):
        """Generate grasping point and affordancemap.

        This function give an affordanceMap and grasping parameters for HANet.

        Args:
          input_color (tensor): color data got from dataloader batch['color'].
          input_depth (tensor): depth data got from dataloader batch['depth'].
          depth_origin (ndarray): depth data got from dataloader batch['depth_origin']

        Returns:
            affordanceMap, grasping 2D coordinate x and y, theta
        """
        with torch.no_grad():
            predict = self.net(input_color, input_depth)
        predict = predict.cpu().detach().numpy()

        Max = []
        Re = []
        Angle = [90,135,0,45]
        height = depth_origin.shape[0]
        width = depth_origin.shape[1]
        re = np.zeros((4, height, width))

        for i in range(4):
            x, y = np.where(predict[0][i] == np.max(predict[0][i]))
            re[i] = cv2.resize(predict[0][i], (width, height))
            Max.append(np.max(predict[0][i]))
            Re.append(re[i])

        theta = Angle[Max.index(max(Max))]
        graspable = re[Max.index(max(Max))]

        graspable = cv2.resize(graspable, (width, height))
        depth = cv2.resize(depth_origin, (width, height))
        graspable [depth==0] = 0
        graspable[graspable>=1] = 0.99999
        graspable[graspable<0] = 0
        graspable = cv2.GaussianBlur(graspable, (7, 7), 0)
        affordanceMap = (graspable/np.max(graspable)*255).astype(np.uint8)
        affordanceMap = cv2.applyColorMap(affordanceMap, cv2.COLORMAP_JET)
        affordanceMap = affordanceMap[:,:,[2,1,0]]

        gray = cv2.cvtColor(affordanceMap, cv2.COLOR_RGB2GRAY)
        blurred = cv2.GaussianBlur(gray, (11, 11), 0)
        binaryIMG = cv2.Canny(blurred, 20, 160)
        contours, _ = cv2.findContours(binaryIMG, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        i = 0
        point_x = 0
        point_y = 0
        cX = 0
        cY = 0
        x = 0
        y = 0

        for c in contours:
            M = cv2.moments(c)
            if(M["m00"]!=0):
                cX = int(M["m10"] / M["m00"])
                cY = int(M["m01"] / M["m00"])
                zc = depth[cY, cX]/1000
                i += 1
                point_x += cX
                point_y += cY

        if i != 0:
            x = int(point_x / i)
            y = int(point_y / i)
        else:
            x, y = np.where(predict[0][Max.index(max(Max))] == np.max(predict[0][Max.index(max(Max))]))
            x = int(x)
            y = int(y)

        return affordanceMap, x, y, theta

    def forward(self, Color, Depth):
        output = self.net(Color, Depth)

        return output

class HANet_depth(nn.Module):
    class FCN_model(nn.Module):
        def __init__(self, n_classes):
            super(HANet_depth.FCN_model, self).__init__()
            self.depth_trunk = torchvision.models.resnet101(pretrained=True)
            del self.depth_trunk.fc, self.depth_trunk.avgpool, self.depth_trunk.layer4

            self.conv1 = nn.Conv2d(1024, 512, 1)
            self.conv2 = nn.Conv2d(512, 128, 1)
            self.conv3 = nn.Conv2d(128, n_classes, 1)
        def forward(self, depth):
            # depth
            depth_feat_1 = self.depth_trunk.conv1(depth) # 3 -> 64
            depth_feat_1 = self.depth_trunk.bn1(depth_feat_1)
            depth_feat_1 = self.depth_trunk.relu(depth_feat_1)
            depth_feat_1 = self.depth_trunk.maxpool(depth_feat_1)
            depth_feat_2 = self.depth_trunk.layer1(depth_feat_1) # 64 -> 256
            depth_feat_3 = self.depth_trunk.layer2(depth_feat_2) # 256 -> 512
            depth_feat_4 = self.depth_trunk.layer3(depth_feat_3) # 512 -> 1024

            feat_1 = self.conv1(depth_feat_4)
            feat_2 = self.conv2(feat_1)
            feat_3 = self.conv3(feat_2)
            return nn.Upsample(scale_factor=2, mode="bilinear")(feat_3)

    def __init__(self, pretrained=False, n_class=4):
        super(HANet_depth, self).__init__()
        if pretrained == True:
            self.net = self.FCN_model(n_classes=4)
            model_path = get_model(True)
            self.net.load_state_dict(torch.load(model_path))
            print('Load pretrained complete')
        else:
            self.net = self.FCN_model(n_classes=n_class)

    def get_affordanceMap(self, input_depth, depth_origin):
        """Generate grasping point and affordancemap.

        This function give an affordanceMap and grasping parameters for HANet.

        Args:
          input_depth (tensor): depth data got from dataloader batch['depth'].
          depth_origin (ndarray): depth data got from dataloader batch['depth_origin']

        Returns:
            affordanceMap, grasping 2D coordinate x and y, theta
        """
        with torch.no_grad():
            predict = self.net(input_depth)
        predict = predict.cpu().detach().numpy()

        Max = []
        Re = []
        Angle = [90,135,0,45]
        height = depth_origin.shape[0]
        width = depth_origin.shape[1]
        re = np.zeros((4, height, width))

        for i in range(4):
            x, y = np.where(predict[0][i] == np.max(predict[0][i]))
            re[i] = cv2.resize(predict[0][i], (width, height))
            Max.append(np.max(predict[0][i]))
            Re.append(re[i])

        theta = Angle[Max.index(max(Max))]
        graspable = re[Max.index(max(Max))]

        graspable = cv2.resize(graspable, (width, height))
        depth = cv2.resize(depth_origin, (width, height))
        graspable [depth==0] = 0
        graspable[graspable>=1] = 0.99999
        graspable[graspable<0] = 0
        graspable = cv2.GaussianBlur(graspable, (7, 7), 0)
        affordanceMap = (graspable/np.max(graspable)*255).astype(np.uint8)
        affordanceMap = cv2.applyColorMap(affordanceMap, cv2.COLORMAP_JET)
        affordanceMap = affordanceMap[:,:,[2,1,0]]

        gray = cv2.cvtColor(affordanceMap, cv2.COLOR_RGB2GRAY)
        blurred = cv2.GaussianBlur(gray, (11, 11), 0)
        binaryIMG = cv2.Canny(blurred, 20, 160)
        contours, _ = cv2.findContours(binaryIMG, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        i = 0
        point_x = 0
        point_y = 0
        cX = 0
        cY = 0
        x = 0
        y = 0

        for c in contours:
            M = cv2.moments(c)
            if(M["m00"]!=0):
                cX = int(M["m10"] / M["m00"])
                cY = int(M["m01"] / M["m00"])
                zc = depth[cY, cX]/1000
                i += 1
                point_x += cX
                point_y += cY

        if i != 0:
            x = int(point_x / i)
            y = int(point_y / i)
        else:
            x, y = np.where(predict[0][Max.index(max(Max))] == np.max(predict[0][Max.index(max(Max))]))
            x = int(x)
            y = int(y)

        return affordanceMap, x, y, theta

    def forward(self, Depth):
        output = self.net(Depth)

        return output

