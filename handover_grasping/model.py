# AUTOGENERATED! DO NOT EDIT! File to edit: 01_model.ipynb (unless otherwise specified).

__all__ = ['HANet', 'GGCNN']

# Cell
import torch
import torch.nn as nn
import torchvision
import copy
import cv2
import numpy as np
from .utils import get_model
from rosenberger_utils.helper_ggcnn.ggcnn import predict
from rosenberger_utils.helper_ggcnn.pre_processing import Preparations

class HANet(nn.Module):
    class FCN_model(nn.Module):
        def __init__(self, n_classes=4):
            super(HANet.FCN_model, self).__init__()
            self.color_trunk = torchvision.models.resnet101(pretrained=True)
            del self.color_trunk.fc, self.color_trunk.avgpool, self.color_trunk.layer4
            self.depth_trunk = copy.deepcopy(self.color_trunk)
            self.conv1 = nn.Conv2d(2048, 512, 1)
            self.conv2 = nn.Conv2d(512, 128, 1)
            self.conv3 = nn.Conv2d(128, n_classes, 1)
        def forward(self, color, depth):
            # Color
            color_feat_1 = self.color_trunk.conv1(color) # 3 -> 64
            color_feat_1 = self.color_trunk.bn1(color_feat_1)
            color_feat_1 = self.color_trunk.relu(color_feat_1)
            color_feat_1 = self.color_trunk.maxpool(color_feat_1)
            color_feat_2 = self.color_trunk.layer1(color_feat_1) # 64 -> 256
            color_feat_3 = self.color_trunk.layer2(color_feat_2) # 256 -> 512
            color_feat_4 = self.color_trunk.layer3(color_feat_3) # 512 -> 1024
            # Depth
            depth_feat_1 = self.depth_trunk.conv1(depth) # 3 -> 64
            depth_feat_1 = self.depth_trunk.bn1(depth_feat_1)
            depth_feat_1 = self.depth_trunk.relu(depth_feat_1)
            depth_feat_1 = self.depth_trunk.maxpool(depth_feat_1)
            depth_feat_2 = self.depth_trunk.layer1(depth_feat_1) # 64 -> 256
            depth_feat_3 = self.depth_trunk.layer2(depth_feat_2) # 256 -> 512
            depth_feat_4 = self.depth_trunk.layer3(depth_feat_3) # 512 -> 1024
            # Concatenate
            feat = torch.cat([color_feat_4, depth_feat_4], dim=1) # 2048
            feat_1 = self.conv1(feat)
            feat_2 = self.conv2(feat_1)
            feat_3 = self.conv3(feat_2)
            return nn.Upsample(scale_factor=2, mode="bilinear")(feat_3)

    def __init__(self, pretrained=False, n_class=4):
        super(HANet, self).__init__()
        if pretrained == True:
            self.net = self.FCN_model(n_classes=4)
            model_path = get_model()
            self.net.load_state_dict(torch.load(model_path))
            print('Load pretrained complete')
        else:
            self.net = self.FCN_model(n_classes=n_class)

    def forward(self, Color, Depth):
        output = self.net(Color, Depth)

        return output

class GGCNN():
    def __init__(self):
        self.prep = Preparations()
        self.scale = 300
        self.out_height = 480
        self.out_width = 640


    def largest_indices(self, array, n):
        """Returns the n largest indices from a numpy array."""
        flat = array.flatten()
        indices = np.argpartition(flat, -n)[-n:]
        indices = indices[np.argsort(-flat[indices])]

        return np.unravel_index(indices, array.shape)

    def angle_translater(self, angle, idx):
        """Remap angle."""
        angle = (angle + np.pi/2) % np.pi - np.pi/2
        angle = angle[idx] *180/np.pi

        return angle


    def pred_grasp(self,depth, depth_nan, mask_body, mask_hand, bbox):
        """Get pixel-wise prediction result of grasping point and angle."""
        depth_bg, mask_bg = self.prep.prepare_image_mask(depth=depth, depth_nan=depth_nan,
                                                    mask_body=mask_body, mask_hand=mask_hand,
                                                    dist_obj=bbox[4], dist_ignore=1.0,
                                                    grip_height=0.08)

        depth_ggcnn = cv2.resize(depth_bg,(self.scale,self.scale))
        mask_ggcnn = cv2.resize(mask_bg,(self.scale,self.scale))

        points, angle, width_img, _ = predict(depth=depth_ggcnn, mask=mask_ggcnn,
                                            crop_size=self.scale, out_size=self.scale,
                                            crop_y_offset=40, filters=(2.0, 2.0, 2.0))

        return points, angle, width_img

    def get_grasp_pose(self, points, angle, width_img, top_n = 1):
        """Get Top-N prediction result

        output : list of Top-N result:[x, y, theta]
        """
        best_g = self.largest_indices(points, top_n)
        out_list = []
        for i in range(top_n):
            best_g_unr = (best_g[0][i], best_g[1][i])
            Angle = self.angle_translater(angle, best_g_unr)
            resize_point = [best_g_unr[0]*(self.out_height/self.scale), best_g_unr[1]*(self.out_width/self.scale)]

            x = int(resize_point[1])
            y = int(resize_point[0])
            theta = Angle

            out_list.append([x, y, theta])

        return out_list
